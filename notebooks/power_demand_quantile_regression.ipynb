{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfd3ae6-0e5e-4f78-9e9f-e2cc688bdee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T14:22:21.149479Z",
     "start_time": "2023-11-22T14:22:21.095196Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dff0f0d8-9316-4510-8e60-b12a0e9ac253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T10:45:52.289536Z",
     "start_time": "2023-11-19T10:45:52.233974Z"
    }
   },
   "outputs": [],
   "source": [
    "from functions import get_energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]/Users/stephantimpe/PycharmProjects/probabilistic-forecasting-challenge/functions/get_energy.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  energydata = pd.concat([energydata, pd.DataFrame(rawdata, columns=col_names)])\n",
      "100%|██████████| 256/256 [00:28<00:00,  9.01it/s]\n"
     ]
    }
   ],
   "source": [
    "df=get_energy.get()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.880560Z",
     "start_time": "2023-11-19T10:45:52.238210Z"
    }
   },
   "id": "ae334634c9012794"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "                       gesamt  weekday\ndate_time                             \n2018-12-24 00:00:00  42.02925        0\n2018-12-24 01:00:00  39.61025        0\n2018-12-24 02:00:00  39.13875        0\n2018-12-24 03:00:00  39.42100        0\n2018-12-24 04:00:00  40.74775        0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gesamt</th>\n      <th>weekday</th>\n    </tr>\n    <tr>\n      <th>date_time</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-12-24 00:00:00</th>\n      <td>42.02925</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2018-12-24 01:00:00</th>\n      <td>39.61025</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2018-12-24 02:00:00</th>\n      <td>39.13875</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2018-12-24 03:00:00</th>\n      <td>39.42100</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2018-12-24 04:00:00</th>\n      <td>40.74775</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.887948Z",
     "start_time": "2023-11-19T10:46:20.883653Z"
    }
   },
   "id": "a01887cd9a82c432"
  },
  {
   "cell_type": "markdown",
   "id": "791b084f-f7ec-4a4a-9947-f292dfcddf4c",
   "metadata": {},
   "source": [
    "Define weekday column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8b96cb2-ec82-433d-9c54-040d0499ca57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.893823Z",
     "start_time": "2023-11-19T10:46:20.888495Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#df[\"time\"] = df.index.strftime(\"%H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf1b27-c033-48a9-8758-ca05bfe91ed8",
   "metadata": {},
   "source": [
    "Lead times are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffc6169f-d45e-42b1-a0c0-a6e30c8cbec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.896317Z",
     "start_time": "2023-11-19T10:46:20.891704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[36, 40, 44, 60, 64, 68]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horizons_def = [36, 40, 44, 60, 64, 68]#[24 + 12*i for i in range(5)]\n",
    "horizons_def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf8917-68b4-4a03-b196-83915cabcb64",
   "metadata": {},
   "source": [
    "Adapt horzions so they actually fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee0221c6-eb42-43de-8ec0-4aa2fd1519bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.898786Z",
     "start_time": "2023-11-19T10:46:20.896028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[37, 41, 45, 61, 65, 69]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horizons = [h+1 for h in horizons_def]\n",
    "horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a4b5b1f-f4a7-45fd-973d-a691a601b2cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.910412Z",
     "start_time": "2023-11-19T10:46:20.900107Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_date_from_horizon(last_ts, horizon):\n",
    "    return last_ts + pd.DateOffset(hours=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "564d7433-a811-40e5-a9ea-2e837b2b2a02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.946551Z",
     "start_time": "2023-11-19T10:46:20.904534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Timestamp('2023-11-19 10:00:00')"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAST_IDX = -1\n",
    "LAST_DATE = df.iloc[LAST_IDX].name\n",
    "LAST_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thursday of the current week: 2023-11-23 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Calculate the number of days to add or subtract to get to Thursday\n",
    "# In Python's datetime module, Monday is 0 and Sunday is 6\n",
    "days_until_thursday = 3 - current_date.weekday()  # 3 represents Thursday\n",
    "\n",
    "# If it's already past Thursday, calculate for the next Thursday\n",
    "if days_until_thursday < 0:\n",
    "    days_until_thursday += 7\n",
    "\n",
    "# Get the Thursday of the current week\n",
    "thursday_of_current_week = current_date + timedelta(days=days_until_thursday)\n",
    "\n",
    "# To set the time to 00:00, replace the hour, minute, second, and microsecond\n",
    "thursday_of_current_week = thursday_of_current_week.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "print(\"Thursday of the current week:\", thursday_of_current_week)\n",
    "LAST_DATE=thursday_of_current_week\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.948188Z",
     "start_time": "2023-11-19T10:46:20.911248Z"
    }
   },
   "id": "9dc06427dfc12345"
  },
  {
   "cell_type": "markdown",
   "id": "51214044-60af-4d25-a508-03c0a16787bc",
   "metadata": {},
   "source": [
    "Get time and date that correspond to the lead times (starting at the last observation in our data which should be the respective thursday 0:00)  \n",
    "*Attention*: if the last timestamp in the data is not thursday 0:00, you have to adjust your lead times accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e0da017-d3b4-4cde-b373-63213b42ebc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.973814Z",
     "start_time": "2023-11-19T10:46:20.916232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Timestamp('2023-11-24 13:00:00'),\n Timestamp('2023-11-24 17:00:00'),\n Timestamp('2023-11-24 21:00:00'),\n Timestamp('2023-11-25 13:00:00'),\n Timestamp('2023-11-25 17:00:00'),\n Timestamp('2023-11-25 21:00:00')]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horizon_date = [get_date_from_horizon(LAST_DATE, h) for h in horizons]\n",
    "horizon_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf62b9-a0ba-4582-9f83-ee24aadb0223",
   "metadata": {},
   "source": [
    "quantile levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf3ae619-5bdc-4c66-8137-4daa880ce9c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.993181Z",
     "start_time": "2023-11-19T10:46:20.925182Z"
    }
   },
   "outputs": [],
   "source": [
    "tau = [.025, .25, .5, .75, .975]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seasonal regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a486092d6a17ef"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "#seasonal regression\n",
    "# Create dummy variables for months and hours\n",
    "df['month'] = df.index.month\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "# Get dummies for months and hours, excluding the first month and hour to avoid multicollinearity\n",
    "month_dummies = pd.get_dummies(df['month'], prefix='month', drop_first=True)\n",
    "hour_dummies = pd.get_dummies(df['hour'], prefix='hour', drop_first=True)\n",
    "\n",
    "# Join the dummies with the original DataFrame\n",
    "df = df.join(month_dummies).join(hour_dummies)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:20.997341Z",
     "start_time": "2023-11-19T10:46:20.933212Z"
    }
   },
   "id": "5d01da2b5cf92c34"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gesamt      float64\n",
      "weekday       int32\n",
      "month         int32\n",
      "hour          int32\n",
      "month_2        bool\n",
      "month_3        bool\n",
      "month_4        bool\n",
      "month_5        bool\n",
      "month_6        bool\n",
      "month_7        bool\n",
      "month_8        bool\n",
      "month_9        bool\n",
      "month_10       bool\n",
      "month_11       bool\n",
      "month_12       bool\n",
      "hour_1         bool\n",
      "hour_2         bool\n",
      "hour_3         bool\n",
      "hour_4         bool\n",
      "hour_5         bool\n",
      "hour_6         bool\n",
      "hour_7         bool\n",
      "hour_8         bool\n",
      "hour_9         bool\n",
      "hour_10        bool\n",
      "hour_11        bool\n",
      "hour_12        bool\n",
      "hour_13        bool\n",
      "hour_14        bool\n",
      "hour_15        bool\n",
      "hour_16        bool\n",
      "hour_17        bool\n",
      "hour_18        bool\n",
      "hour_19        bool\n",
      "hour_20        bool\n",
      "hour_21        bool\n",
      "hour_22        bool\n",
      "hour_23        bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "print(df.dtypes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:21.009575Z",
     "start_time": "2023-11-19T10:46:20.952733Z"
    }
   },
   "id": "8eee6b107e342a00"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mapply(pd\u001B[38;5;241m.\u001B[39mto_numeric)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Train a model for each quantile\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m models \u001B[38;5;241m=\u001B[39m {q: sm\u001B[38;5;241m.\u001B[39mQuantReg(y, X)\u001B[38;5;241m.\u001B[39mfit(q\u001B[38;5;241m=\u001B[39mq) \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m quantiles}\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Prepare your forecast DataFrame - assuming 'forecast_df' is your forecast DataFrame\u001B[39;00m\n\u001B[1;32m     18\u001B[0m forecast_df_for_prediction \u001B[38;5;241m=\u001B[39m forecast_df\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[0;32mIn[36], line 15\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     12\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mapply(pd\u001B[38;5;241m.\u001B[39mto_numeric)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Train a model for each quantile\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m models \u001B[38;5;241m=\u001B[39m {q: \u001B[43msm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mQuantReg\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfit(q\u001B[38;5;241m=\u001B[39mq) \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m quantiles}\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Prepare your forecast DataFrame - assuming 'forecast_df' is your forecast DataFrame\u001B[39;00m\n\u001B[1;32m     18\u001B[0m forecast_df_for_prediction \u001B[38;5;241m=\u001B[39m forecast_df\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/Forecasting_Challenge_2/lib/python3.9/site-packages/statsmodels/regression/quantile_regression.py:79\u001B[0m, in \u001B[0;36mQuantReg.__init__\u001B[0;34m(self, endog, exog, **kwargs)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, endog, exog, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_kwargs(kwargs)\n\u001B[0;32m---> 79\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mQuantReg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Forecasting_Challenge_2/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:202\u001B[0m, in \u001B[0;36mRegressionModel.__init__\u001B[0;34m(self, endog, exog, **kwargs)\u001B[0m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, endog, exog, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 202\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mRegressionModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpinv_wexog: Float64Array \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_attr\u001B[38;5;241m.\u001B[39mextend([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpinv_wexog\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwendog\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwexog\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweights\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/Forecasting_Challenge_2/lib/python3.9/site-packages/statsmodels/base/model.py:270\u001B[0m, in \u001B[0;36mLikelihoodModel.__init__\u001B[0;34m(self, endog, exog, **kwargs)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, endog, exog\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 270\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitialize()\n",
      "File \u001B[0;32m~/anaconda3/envs/Forecasting_Challenge_2/lib/python3.9/site-packages/statsmodels/base/model.py:95\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, endog, exog, **kwargs)\u001B[0m\n\u001B[1;32m     93\u001B[0m missing \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmissing\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     94\u001B[0m hasconst \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhasconst\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhasconst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[43m                              \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_constant \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mk_constant\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexog \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mexog\n",
      "File \u001B[0;32m~/anaconda3/envs/Forecasting_Challenge_2/lib/python3.9/site-packages/statsmodels/base/model.py:135\u001B[0m, in \u001B[0;36mModel._handle_data\u001B[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_handle_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, endog, exog, missing, hasconst, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 135\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mhandle_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhasconst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001B[39;00m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m kwargs:\n",
      "File \u001B[0;32m~/anaconda3/envs/Forecasting_Challenge_2/lib/python3.9/site-packages/statsmodels/base/data.py:675\u001B[0m, in \u001B[0;36mhandle_data\u001B[0;34m(endog, exog, missing, hasconst, **kwargs)\u001B[0m\n\u001B[1;32m    672\u001B[0m     exog \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(exog)\n\u001B[1;32m    674\u001B[0m klass \u001B[38;5;241m=\u001B[39m handle_data_class_factory(endog, exog)\n\u001B[0;32m--> 675\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mklass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhasconst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhasconst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m             \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Forecasting_Challenge_2/lib/python3.9/site-packages/statsmodels/base/data.py:84\u001B[0m, in \u001B[0;36mModelData.__init__\u001B[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001B[0m\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morig_endog \u001B[38;5;241m=\u001B[39m endog\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morig_exog \u001B[38;5;241m=\u001B[39m exog\n\u001B[0;32m---> 84\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mendog, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexog \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_endog_exog\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendog\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexog\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconst_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_constant \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/Forecasting_Challenge_2/lib/python3.9/site-packages/statsmodels/base/data.py:509\u001B[0m, in \u001B[0;36mPandasData._convert_endog_exog\u001B[0;34m(self, endog, exog)\u001B[0m\n\u001B[1;32m    507\u001B[0m exog \u001B[38;5;241m=\u001B[39m exog \u001B[38;5;28;01mif\u001B[39;00m exog \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(exog)\n\u001B[1;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m endog\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m exog \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m exog\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m:\n\u001B[0;32m--> 509\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPandas data cast to numpy dtype of object. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    510\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCheck input data with np.asarray(data).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(PandasData, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m_convert_endog_exog(endog, exog)\n",
      "\u001B[0;31mValueError\u001B[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the quantiles you're interested in\n",
    "quantiles = [.025, .25, .5, .75, .975]\n",
    "\n",
    "# Prepare your features (X) and target (y) variables\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target vector\n",
    "exclude_columns = ['gesamt', 'month', 'hour']\n",
    "X = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "y = df['gesamt']\n",
    "X = X.apply(pd.to_numeric)\n",
    "y = y.apply(pd.to_numeric)\n",
    "\n",
    "# Train a model for each quantile\n",
    "models = {q: sm.QuantReg(y, X).fit(q=q) for q in quantiles}\n",
    "\n",
    "# Prepare your forecast DataFrame - assuming 'forecast_df' is your forecast DataFrame\n",
    "forecast_df_for_prediction = forecast_df.drop('date_time', axis=1)\n",
    "\n",
    "# Make predictions for each quantile\n",
    "for q, model in models.items():\n",
    "    forecast_df[f'prediction_q{q}'] = model.predict(forecast_df_for_prediction)\n",
    "\n",
    "# Now 'forecast_df' contains predictions for each quantile\n",
    "print(forecast_df[['date_time'] + [f'prediction_q{q}' for q in quantiles]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:21.302780Z",
     "start_time": "2023-11-19T10:46:20.960552Z"
    }
   },
   "id": "e7ce2d3ef478a4ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#seasonal regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the independent variables (X) and the dependent variable (y)\n",
    "# Exclude the original 'month' and 'hour' columns to avoid multicollinearity\n",
    "exclude_columns = ['gesamt', 'month', 'hour']\n",
    "X = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "y = df['gesamt']\n",
    "X = X.apply(pd.to_numeric)\n",
    "y = y.apply(pd.to_numeric)\n",
    "\n",
    "for column in X.columns:\n",
    "    X[column] = X[column].astype(int)\n",
    "\n",
    "\n",
    "# Add a constant to the model (for the intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())\n",
    "'quantile reg'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T10:46:21.307848Z",
     "start_time": "2023-11-19T10:46:21.302961Z"
    }
   },
   "id": "cf58f17ff1c61753"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming horizon_dates is a list of pandas Timestamps\n",
    "horizon_dates = [pd.Timestamp('2023-11-17 13:00:00'),\n",
    "                 pd.Timestamp('2023-11-17 17:00:00'),\n",
    "                 pd.Timestamp('2023-11-17 21:00:00'),\n",
    "                 pd.Timestamp('2023-11-18 13:00:00'),\n",
    "                 pd.Timestamp('2023-11-18 17:00:00'),\n",
    "                 pd.Timestamp('2023-11-18 21:00:00')]\n",
    "\n",
    "# Create a new DataFrame for the forecast\n",
    "forecast_df = pd.DataFrame({'date_time': horizon_dates})\n",
    "# Extract and one-hot encode month and hour from the date_time\n",
    "forecast_df['month'] = forecast_df['date_time'].dt.month\n",
    "forecast_df['hour'] = forecast_df['date_time'].dt.hour\n",
    "forecast_df = forecast_df.join(pd.get_dummies(forecast_df['month'], prefix='month', drop_first=True))\n",
    "forecast_df = forecast_df.join(pd.get_dummies(forecast_df['hour'], prefix='hour', drop_first=True))\n",
    "\n",
    "# Drop the original month and hour columns\n",
    "forecast_df.drop(['month', 'hour'], axis=1, inplace=True)\n",
    "\n",
    "# Add constant and weekday columns\n",
    "forecast_df['const'] = 1.0\n",
    "forecast_df['weekday'] = forecast_df['date_time'].dt.dayofweek\n",
    "\n",
    "# Ensure all columns in X are also in forecast_df\n",
    "for col in X.columns:\n",
    "    if col not in forecast_df.columns and col != 'date_time':\n",
    "        forecast_df[col] = 0\n",
    "\n",
    "# Reorder columns to match X (excluding date_time)\n",
    "forecast_columns = [col for col in X.columns if col != 'date_time']\n",
    "forecast_df = forecast_df[['date_time'] + forecast_columns]\n",
    "\n",
    "# Now forecast_df is ready for prediction\n",
    "# Drop the date_time column for prediction\n",
    "forecast_df_for_prediction = forecast_df.drop('date_time', axis=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(forecast_df_for_prediction)\n",
    "\n",
    "# Combine predictions with date_times\n",
    "forecast_df['prediction'] = predictions\n",
    "print(forecast_df[['date_time', 'prediction']])\n",
    "forecast_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.305509Z"
    }
   },
   "id": "cffd5957a3fe89e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.307181Z"
    }
   },
   "id": "f597869925e046e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.309124Z"
    }
   },
   "id": "c5028ea49ea1e75f"
  },
  {
   "cell_type": "markdown",
   "id": "420cfb8d-f64b-46b1-88e3-8d5159eb88bc",
   "metadata": {},
   "source": [
    "Visually check if quantiles make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9b394-79d2-48dd-914b-8fc9a9f1a5a1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.310373Z"
    }
   },
   "outputs": [],
   "source": [
    "x = horizons\n",
    "_ = plt.plot(x,pred_baseline, ls=\"\", marker=\"o\", c=\"black\")\n",
    "_ = plt.xticks(x, x)\n",
    "_ = plt.plot((x,x),(pred_baseline[:,0], pred_baseline[:,-1]),c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8c6fe-43c9-4a71-b8f0-300a382f59c6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.311945Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "date_str = datetime.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024fd0b-f3d5-40f4-81cd-5d2d6f5d4237",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.312958Z"
    }
   },
   "outputs": [],
   "source": [
    "date_str = date.today() #- timedelta(days=1)\n",
    "date_str = date_str\n",
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ff112-6973-4e8c-be11-5705434c3416",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.313738Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "    \"forecast_date\": date_str,\n",
    "    \"target\": \"energy\",\n",
    "    \"horizon\": [str(h) + \" hour\" for h in horizons_def],\n",
    "    \"q0.025\": pred_baseline[:,0],\n",
    "    \"q0.25\": pred_baseline[:,1],\n",
    "    \"q0.5\": pred_baseline[:,2],\n",
    "    \"q0.75\": pred_baseline[:,3],\n",
    "    \"q0.975\": pred_baseline[:,4]})\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f92f2c-859a-4094-8bb4-3249c4de4f2b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.314518Z"
    }
   },
   "outputs": [],
   "source": [
    "#need to change this\n",
    "PATH = \"../forecasts/\"\n",
    "date_str = date_str.strftime('%Y%m%d')\n",
    "\n",
    "df_sub.to_csv(PATH+date_str+\"_power_benchmark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87e79b-7c52-4b83-8848-154e90df9323",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-19T10:46:21.315407Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
